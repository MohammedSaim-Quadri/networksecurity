# Network Security project for Phishing Dataset

## Overview

This is a full-scale MLOps project that implements an end-to-end machine learning pipeline for network security applications. The project covers the complete ML lifecycle, including data ingestion, preprocessing, model training, evaluation, deployment, and continuous monitoring using cloud platforms like AWS.

### Features
- Automated Data Ingestion from structured network datasets\
- Data Preprocessing & Validation using defined schemas
- Model Training & Evaluation using state-of-the-art ML algorithms
- Cloud Integration with AWS S3 for model storage and retrieval
- CI/CD Pipeline for continuous integration and deployment
- Dockerization for seamless deployment
- Batch Prediction Pipeline for inference on new data
- Logging & Exception Handling for debugging and monitoring

### Project Structure
```bash
â”œâ”€â”€ mohammedsaim-quadri-networksecurity/
â”‚   â”œâ”€â”€ README.md                # Project Documentation
â”‚   â”œâ”€â”€ app.py                   # Main Application Entry Point
â”‚   â”œâ”€â”€ Dockerfile               # Docker Configuration for Deployment
â”‚   â”œâ”€â”€ main.py                  # Training Pipeline Execution
â”‚   â”œâ”€â”€ push_data.py             # Data Pusher for Cloud Storage
â”‚   â”œâ”€â”€ requirements.txt         # Dependencies
â”‚   â”œâ”€â”€ setup.py                 # Package Setup
â”‚   â”œâ”€â”€ test_mongo.py            # MongoDB Connectivity Testing
â”‚   â”œâ”€â”€ data_schema/
â”‚   â”‚   â””â”€â”€ schema.yaml          # Data Schema Definitions
â”‚   â”œâ”€â”€ final_models/            # Trained Models
â”‚   â”‚   â”œâ”€â”€ model.pkl            # Final ML Model
â”‚   â”‚   â””â”€â”€ preprocessor.pkl     # Data Preprocessing Pipeline
â”‚   â”œâ”€â”€ Network_Data/            # Raw Network Datasets
â”‚   â”‚   â””â”€â”€ phisingData.csv
â”‚   â”œâ”€â”€ networksecurity/         # Core Codebase
â”‚   â”‚   â”œâ”€â”€ cloud/               # AWS S3 Integration
â”‚   â”‚   â”œâ”€â”€ components/          # ML Pipeline Components
â”‚   â”‚   â”œâ”€â”€ constant/            # Constants & Configs
â”‚   â”‚   â”œâ”€â”€ entity/              # Entity Definitions
â”‚   â”‚   â”œâ”€â”€ exception/           # Custom Exception Handling
â”‚   â”‚   â”œâ”€â”€ logging/             # Logging Mechanisms
â”‚   â”‚   â”œâ”€â”€ pipeline/            # Training & Prediction Pipelines
â”‚   â”‚   â””â”€â”€ utils/               # Utility Functions
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â””â”€â”€ table.html           # UI Component
â”‚   â”œâ”€â”€ valid_data/
â”‚   â”‚   â””â”€â”€ test.csv             # Validated Test Data
â”‚   â””â”€â”€ .github/workflows/
â”‚       â””â”€â”€ main.yml             # GitHub Actions Workflow
```

```mermaid
%%{init: {'theme': 'base', 'themeVariables': {'primaryColor': '#bbdefb', 'edgeLabelBackground':'#ffffff'}}}%%

graph TD;
    
    %% Data Pipeline %%
    A(["ðŸ—„ï¸ Data Collection"]) -->|Ingest Data| B(["ðŸ“¥ Data Ingestion"])
    B -->|Validate Data| C(["âœ… Data Validation"])
    C -->|Transform Data| D(["ðŸ”„ Data Transformation"])
    D -->|Train Model| E(["ðŸ¤– Model Training"])
    E -->|Evaluate Model| F(["ðŸ“Š Model Evaluation"])
    F -->|Store Model| G(["ðŸ“ Model Registry (S3)"])

    %% Deployment %%
    G -->|Deploy Model| H(["ðŸš€ Model Deployment"])
    H -->|Serve Predictions| I(["ðŸŒ Web Service (Flask API)"])
    I -->|User Requests| J(["ðŸ–¥ï¸ Frontend/Table UI"])

    %% MLOps & Monitoring %%
    G -->|Monitor Model| K(["ðŸ“¡ Model Monitoring"])
    K -->|Trigger Retraining| D

    %% Subgraphs for Organization %%
    subgraph "ðŸ”§ MLOps Pipeline"
      B
      C
      D
      E
      F
      G
      H
      K
    end

    subgraph "â˜ï¸ Cloud Storage"
      L(["ðŸ—‚ï¸ AWS S3"])
      G --> L
      H --> L
    end

    subgraph "ðŸ› ï¸ CI/CD Pipeline"
      M(["âš™ï¸ GitHub Actions"])
      M -->|Automate Deployment| H
    end
```
## Installation & Setup

### Prerequisites
- Python 3.8+
- Docker
- AWS Account & CLI Setup
- GitHub Actions for CI/CD

### Install Dependencies
```bash
pip install -r requirements.txt
```
### Run Model Training
```bash
python main.py
```
### Run Application Locally
```bash
python app.py
```
### Build & Run Docker Container
```bash
docker build -t networksecurity-app .
docker run -p 5000:5000 networksecurity-app
```
## CI/CD & Deployment

### GitHub Actions Workflow (.github/workflows/main.yml)

This project implements Continuous Integration (CI) and Continuous Deployment (CD) using GitHub Actions:
- Linting & Unit Testing
- Building and Pushing Docker Image to AWS ECR
- Deploying Model & Application

### AWS Integration
- S3 Bucket: Stores model artifacts
- ECR: Stores containerized app

## Future Improvements
- Implement Model Drift Detection
- Integrate Live Monitoring with Prometheus/Grafana
- Enable AutoML for Hyperparameter Tuning
- Expand to Real-time Threat Detection
